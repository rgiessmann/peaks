# coding: utf-8

import logging

# In[1]:
# -------------------------------- #
### set plotting options
#get_ipython().magic(u'matplotlib Agg')
# -------------------------------- #

## SELECT YOUR LOGGING LEVEL ##
_logging_level = "DEBUG"
#_logging_level = "INFO"
#_logging_level = "WARNING"


#_logging_level = "CRITICAL"
## ------------------------  ##
import logging
_level = eval("logging."+_logging_level)
logging.basicConfig(level=_level)
logger = logging.getLogger(__name__)


# In[2]:
# -------------------------------- #
## import code for data processing
import peaks.footprint
footprint = peaks.footprint.Footprinter() 
# -------------------------------- #


# In[3]:
# -------------------------------- #
### Import metadata for 'Peak Scanner 2' output
trace_list = footprint.get_data("./input_traces.csv")
# -------------------------------- #


# In[21]:
# -------------------------------- #
### Set options
## accepted_offset: peaks are clustered, if their size (=bp) lies within this difference (in bp)
## factor_method: if "num" look for optimal factor (free-floating), if "peak" find a footprinting-insensitive peak
## weight_smaller/weight_bigger: importance of misfit peaks which are smaller / bigger than reference peaks
## relative_mode: if "True" determine the importance of misfit peaks due to their relative size compared to the reference peak, if "False" consider absolute difference (in AU)
## from_bp/ to_bp: peaks in this interval of sizes (=bp) shall be considered for analysis.


_accepted_offset = 0.3
_factor_method   = "num"
_weight_smaller  = 1
_weight_bigger   = 1
_relative_mode   = True
_from_bp         = float("-inf")
_to_bp           = float("+inf")
_normalize_to    = None
_how_many_peaks_necessary = -1

### Generate the reference trace
ref = footprint.generate_averaged_negative_control(trace_list,accepted_offset=_accepted_offset,factor_method=_factor_method, weight_smaller=0.5, weight_bigger=0.5, relative_mode=_relative_mode, from_bp=_from_bp, to_bp=_to_bp, normalize_to=None, how_many_peaks_necessary=_how_many_peaks_necessary)
# -------------------------------- #


# In[22]:
# -------------------------------- #
footprint.cluster_peaks(ref,trace_list,accepted_offset=_accepted_offset)


# In[22]:
# -------------------------------- #
trace_list_plus = [ref]+trace_list
for trace in trace_list_plus:
    footprint.prune_tracepeaks_to_peaks_present_in_other_traces(trace, trace_list,\
                                         how_many=_how_many_peaks_necessary)
# -------------------------------- #



# -------------------------------- #
if _factor_method   == "peak":
        optimal_factors = footprint.determine_factor_single_peak(ref, trace_list, weight_smaller=_weight_smaller, weight_bigger=_weight_bigger, relative_mode=_relative_mode, from_bp=_from_bp, to_bp=_to_bp)
elif _factor_method  == "num":
        optimal_factors = footprint.determine_factor_numerically(ref, trace_list, weight_smaller=_weight_smaller, weight_bigger=_weight_bigger, relative_mode=_relative_mode, from_bp=_from_bp, to_bp=_to_bp)


### Correct traces with optimal factors 
for index, null in enumerate(trace_list):
    footprint.correct_peaks_with_factor(trace_list[index],optimal_factors[index])
    print("file: {:30.30} --> factor: {:4.2f}".format(trace_list[index].file_name, optimal_factors[index]))
# -------------------------------- #




# In[15]:
# -------------------------------- #
### Calculate fractional occupancy for all footprinted peaks, set "free" ligand concentration to total ligand concentration
footprint.mark_footprinted_peaks(ref, trace_list, threshold=0.01, mark_all=True) 
footprint.add_fractional_occupancies(ref,trace_list)
# -------------------------------- #

# In[30]:
# -------------------------------- #
footprint.fit_all_traces_to_one(ref, trace_list, factor_method="num",  weight_smaller=_weight_smaller, weight_bigger=_weight_bigger, relative_mode=_relative_mode, from_bp=_from_bp, to_bp=_to_bp)
# -------------------------------- #


df = footprint.fit_data_peaks_sensitive(ref, trace_list)
df.to_csv("check-insensitive-peaks.csv")
