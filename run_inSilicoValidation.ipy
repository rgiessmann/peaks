# -*- coding: utf-8 -*-
"""
Created on Sun Apr  3 17:57:07 2016

@author: rgiessmann
"""

# coding: utf-8

import logging
import peaks.insilico as insilico
import pandas
import numpy

insilico.__init__()

# In[1]:
# -------------------------------- #
### set plotting options
#get_ipython().magic(u'matplotlib Agg')
# -------------------------------- #

## SELECT YOUR LOGGING LEVEL ##
_logging_level = "DEBUG"
#_logging_level = "INFO"
#_logging_level = "WARNING"


#_logging_level = "CRITICAL"
## ------------------------  ##
import logging
_level = eval("logging."+_logging_level)
logging.basicConfig(level=_level)
logger = logging.getLogger(__name__)


# In[2]:
# -------------------------------- #
## import code for data processing
import peaks.footprint
footprint = peaks.footprint.Footprinter() 
# -------------------------------- #


# In[3]:
# -------------------------------- #
### Import metadata for 'Peak Scanner 2' output
trace_list = insilico.get_data("./kd-matrix-expected.csv", "./input_traces.csv")
# -------------------------------- #


# In[21]:
# -------------------------------- #
### Set options
## accepted_offset: peaks are clustered, if their size (=bp) lies within this difference (in bp)
## factor_method: if "num" look for optimal factor (free-floating), if "peak" find a footprinting-insensitive peak
## weight_smaller/weight_bigger: importance of misfit peaks which are smaller / bigger than reference peaks
## relative_mode: if "True" determine the importance of misfit peaks due to their relative size compared to the reference peak, if "False" consider absolute difference (in AU)
## from_bp/ to_bp: peaks in this interval of sizes (=bp) shall be considered for analysis.


_accepted_offset = 0.3
_factor_method   = "num"
_weight_smaller  = 1
_weight_bigger   = 1
_relative_mode   = True
_from_bp         = float("-inf")
_to_bp           = float("+inf")
_normalize_to    = None
_how_many_peaks_necessary = -1

### Generate the reference trace
ref = footprint.generate_averaged_negative_control(trace_list,accepted_offset=_accepted_offset,factor_method=_factor_method, weight_smaller=0.5, weight_bigger=0.5, relative_mode=_relative_mode, from_bp=_from_bp, to_bp=_to_bp, normalize_to=None, how_many_peaks_necessary=_how_many_peaks_necessary)
# -------------------------------- #


# In[22]:
# -------------------------------- #
footprint.cluster_peaks(ref,trace_list,accepted_offset=_accepted_offset)


# In[22]:
# -------------------------------- #
#trace_list_plus = [ref]+trace_list
#for trace in trace_list_plus:
#    footprint.prune_tracepeaks_to_peaks_present_in_other_traces(trace, trace_list,\
#                                         how_many=_how_many_peaks_necessary)
# -------------------------------- #



# In[30]:
# -------------------------------- #
footprint.fit_all_traces_to_one(ref, trace_list, factor_method=_factor_method,  weight_smaller=_weight_smaller, weight_bigger=_weight_bigger, relative_mode=_relative_mode, from_bp=_from_bp, to_bp=_to_bp)
# -------------------------------- #




# In[15]:
# -------------------------------- #
### Calculate fractional occupancy for all footprinted peaks, set "free" ligand concentration to total ligand concentration
footprint.mark_footprinted_peaks(ref, trace_list, threshold=0.01, mark_all=True) 
footprint.add_fractional_occupancies(ref,trace_list)
# -------------------------------- #

kd_matrix = footprint.fit_data_determine_kd(ref, trace_list)


kdmat_expected = insilico.read_kdmatrix("kd-matrix-expected.csv")
kdvalues_expected = insilico.get_kd_values(kdmat_expected)

pmatrix = footprint.fit_data_peaks_sensitive(ref, trace_list)

classification_df = insilico.return_binary_classification_success(kdvalues_expected, pmatrix, alpha=0.05)

statistics_dict = insilico.analyze_binary_classification_success(classification_df)

statistics_dict.update( { "alpha" : 0.05 } )

def listify_dict_items(foo):
    for key in foo.keys():
        foo[key] = [ foo[key] ]
    return

listify_dict_items(statistics_dict)


df_original = pandas.DataFrame.from_dict(statistics_dict)






import scipy.optimize

def cost_function(x):
    alpha= x #x[0]

    classification_df = insilico.return_binary_classification_success(kdvalues_expected, pmatrix, alpha)
    statistics_dict = insilico.analyze_binary_classification_success(classification_df)
    
    MCC = statistics_dict["MCC"]
    cost = 1 - MCC

    if numpy.isnan(cost):
        cost = float("inf")

    ##DEBUG
    _str = "x: {}, cost: {}, MCC: {}".format(x,cost,MCC)
    logger.debug(_str)

    return cost

def cost_function_logspace(x):
    x = 10 ** x
    return cost_function(x)

x0 = [0.05]
_bounds = [ (0,1) ]

#_result = scipy.optimize.differential_evolution( cost_function, bounds=_bounds )

_ranges = [ slice(-7, 1, 1e-1) ]
_result = scipy.optimize.brute( func=cost_function_logspace, ranges=_ranges )

optimal_alpha = 10 ** _result[0]
opt_classification_df = insilico.return_binary_classification_success(kdvalues_expected, pmatrix, optimal_alpha)
opt_statistics_dict = insilico.analyze_binary_classification_success(opt_classification_df)
opt_statistics_dict.update( { "alpha" : optimal_alpha } )

listify_dict_items(opt_statistics_dict)

df_optimized = pandas.DataFrame.from_dict(opt_statistics_dict)

df_out = df_original.append(df_optimized)

df_out.to_csv("statistics.csv")


